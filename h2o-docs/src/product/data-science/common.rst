分位数
---------

**注意**: Flow中的分位数结果按需延迟计算并缓存。对于大多数用例来说，这是一个非常精确的快速近似（max - min / 1024）。如果分布是偏态的，分位数结果可能不如在R中使用 ``h2o.quantile`` 或Python中使用 ``H2OFrame.quantile`` 精确。

早停
--------------

所有的AIR监督学习算法都允许在模型构建和评分过程中尽早停止。

所有有监督算法中的早停
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- :ref:`max_runtime_secs` (默认为 0/disabled)

``max_runtime_secs`` 选项指定要分配的最大运行时(以秒为单位)，以便完成模型。如果在模型构建完成之前超过了这个最大运行时，那么模型就会失败。执行网格搜索时，此选项指定整个网格的最大运行时(以秒为单位)。 在模型参数中该选项也可以和 ``max_runtime_secs`` 结合使用。如果 ``max_runtime_secs`` 没有在模型参数中设置，则启动每个模型构建时的限制为网格剩余时间。另一方面，如果在模型参数中设置了 ``max_runtime_secs``，则每次启动构建时的限制都等于模型时间限制的最小值和网格的剩余时间。

AutoML、网格搜索、深度学习、DRF、GBM和XGBoost中的早停
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

在AutoML、网格搜索、深度学习、DRF、GBM和XGBoost中以下附加参数用于提前停止：

- :ref:`stopping_rounds` (在AutoML中默认为3，深度学习中默认为5，在DRF、GBM、XGBoost中默认为0/disabled)
- :ref:`stopping_tolerance` (默认为0.001。 在AutoML中，对于超过100万行的数据集，这默认为较大的值，由数据集的大小和非NA率决定)
- :ref:`stopping_metric` (分类默认为 "logloss" ，回归默认为 "deviance")

在这些算法中，启动提前停止的最简单方法是在 ``stopping_rounds`` 选项中使用 number >=1。其他两个参数的默认值可以很好地工作，但是对于 ``stopping_tolerance`` 0是默认值的常见替代。

此外，在使用这些早期停止方法时，要考虑 :ref:`score_tree_interval` 和/或  :ref:`score_each_iteration`。停止循环适用于AIR已经执行的计分迭代的数量，因此，较小规模的常规计分迭代可以帮助控制尽早停止的次数最多(尽管为了更频繁地计分，需要在速度上进行权衡)。默认情况下是使用AIR的对一个合理的训练时间与评分时间的比例的评价，这常常导致不一致的得分差距。

广义线性模型中的早停
~~~~~~~~~~~~~~~~~~~~~

在广义线性模型中，以下附加参数用于提前停止：

- :ref:`early_stopping` (默认启用)
- :ref:`max_active_predictors` (默认值可以根据求解程序的不同而变化)

当 ``early_stopping`` 被启用， 当对训练集或验证集(如果提供)没有更多的相对改进时，GLM将自动停止构建模型。当没有更多的改进发生时，这个选项可以防止使用许多预测因子构建耗时的模型。

``max_active_predictors`` 选项限制活跃预测因子的数量。（注意，模型中非零预测因子的实际数量会稍微低一些）。这在获得稀疏解时非常有用，可以避免使用过多的预测因子对模型进行耗时的计算。当使用 :math:`\lambda_1` 惩罚和搜索时，此选项将在搜索完成之前停止搜索。在lambda搜索开始时构建的模型具有更高的lambda值，考虑更少的预测因子，并且花费更少的时间来计算模型。为lambda搜索设置 ``nlambdas`` 参数，以指定整个搜索中尝试的模型数量。 

